{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIS2lOG0mKlk"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline       # import packages and libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = stopwords.words('russian')\n",
        "stop_words.extend(['наш', 'ваш', 'твой', 'свой', 'это'])\n",
        "stop_words.remove('не') # exclude 'не' and 'нельзя' from stop_words list\n",
        "stop_words.remove('нельзя')\n",
        "np.random.seed(5)\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow version:', tf.__version__)\n",
        "print('GPU detected:', tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_lg "
      ],
      "metadata": {
        "id": "lAHnHV_ksfdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing Steps"
      ],
      "metadata": {
        "id": "SwVRMiGbIFlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"path_to_data\", encoding = 'utf-8', sep = \",\") # load dataset\n",
        "data = data.fillna(method=\"ffill\") # fill NA values\n",
        "data.head(7) # show first 7 rows"
      ],
      "metadata": {
        "id": "l16bCSnWbqFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " data[\"Tag\"] = data[\"Tag\"].apply(lambda row: re.sub(r'^[BI]_', '', row)) # remove BIO annotation\n",
        "data[\"Tag\"].value_counts() # show tags' distribution"
      ],
      "metadata": {
        "id": "O25HNr2F3Jlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ru_core_news_lg\", disable=['parser', 'ner']) # load big model"
      ],
      "metadata": {
        "id": "pjULNNxls9OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Word\"] = data[\"Word\"].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)])) #lemmatize words\n",
        "data.head(5) # show first 5 lemmatized words"
      ],
      "metadata": {
        "id": "mN-rx3BTsD3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"is_notstop\"] = data[\"Word\"].apply(lambda row: row not in stop_words) # find words that are not in stop_word list\n",
        "data = data.loc[data.is_notstop == True] # keep only not stop_words\n",
        "data = data[['id', 'Word', 'Tag']]"
      ],
      "metadata": {
        "id": "VN-oKT4psEA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique id in corpus:\", data['id'].nunique())\n",
        "print(\"Unique words in corpus:\", data['Word'].nunique())"
      ],
      "metadata": {
        "id": "jVMkO2Z3qhwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = set(list(data['Word'].values)) # create list of words\n",
        "words.add('PADword') # add pads to list of words\n",
        "n_words = len(words) # count number of unique words\n",
        "n_words"
      ],
      "metadata": {
        "id": "ZNsKofMHlgVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = list(set(data[\"Tag\"].values)) # create list of tags\n",
        "tags.sort() # sort tags in list of tags\n",
        "n_tags = len(tags) # count number of unique tags\n",
        "n_tags"
      ],
      "metadata": {
        "id": "VzZOPBBzlgYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm # building vocabulary\n",
        "def sentence_integrate(data):\n",
        "  agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), s[\"Tag\"].values.tolist())] \n",
        "  return data.groupby('id').apply(agg_func).tolist()"
      ],
      "metadata": {
        "id": "t785gBOwlgbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=sentence_integrate(data)\n",
        "sentences[1]"
      ],
      "metadata": {
        "id": "xL4hdMczlgec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags2index = {t:i for i,t in enumerate(tags)} # get indexes for tags"
      ],
      "metadata": {
        "id": "UPaqa8aQqph8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 60 # set maximum length\n",
        "X = [[w[0] for w in s] for s in sentences] # save words as X-axis\n",
        "new_X = [] # if text review is shorter than 60 words then add pads\n",
        "for seq in X:\n",
        "    new_seq = []\n",
        "    for i in range(max_len):\n",
        "        try:\n",
        "            new_seq.append(seq[i])\n",
        "        except:\n",
        "            new_seq.append(\"PADword\")\n",
        "    new_X.append(new_seq)\n",
        "#new_X[1]"
      ],
      "metadata": {
        "id": "8zjTMQn8rFnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "y = [[[tags2index[w[1]]] for w in s] for s in sentences] # save tags indexes as y-axis\n",
        "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tags2index[\"O\"]) # add padword's tag (O) if the sequence is shorter than 60 \n",
        "#y[1]"
      ],
      "metadata": {
        "id": "ybt5EKeVrVWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained ELMo Usage (Embeddings Extraction)"
      ],
      "metadata": {
        "id": "HGLYS6Cgrhx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "5uIfl-lt066c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade simple_elmo # install simple_elmo"
      ],
      "metadata": {
        "id": "dkaqiVkJ07Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simple_elmo import ElmoModel\n",
        "model = ElmoModel()"
      ],
      "metadata": {
        "id": "wG5za-Ew1vE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load(\"path_to_elmo\") # load model"
      ],
      "metadata": {
        "id": "81236OjA1vKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "elmo_vectors = model.get_elmo_vectors(new_X, layers = 'average') # get embeddings for words\n",
        "end = time.time()\n",
        "\n",
        "processing_time = int(end - start)\n",
        "\n",
        "print(f\"ELMo embeddings for your input are ready in {processing_time} seconds\")\n",
        "print(f\"Tensor shape: {elmo_vectors.shape}\")\n"
      ],
      "metadata": {
        "id": "YUn6Y-ye1vOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elmo_vectors[0] # show embeddings for 1st review"
      ],
      "metadata": {
        "id": "94ouuEe-1--B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "v6ZlG6hrWAzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(elmo_vectors, y, test_size=0.2, random_state=1) # split data into train and test sets"
      ],
      "metadata": {
        "id": "HQ3j4YjHr8gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, Flatten\n",
        "from tensorflow.keras.layers import InputLayer, TimeDistributed, Dropout, Bidirectional\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "tg2i71Aer8mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "5lZG7Sb6RgOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "8yqQYqLqQsUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = keras.Sequential()\n",
        "model_lstm.add(InputLayer(input_shape=(60,1024)))   # create model (3rd architecture)\n",
        "model_lstm.add(Bidirectional(LSTM(1024, return_sequences=True)))\n",
        "model_lstm.add(Dense(512, activation = 'relu'))\n",
        "# model_lstm.add(Dropout(0.1))\n",
        "model_lstm.add(Dense(8, activation = 'softmax'))\n",
        "\n",
        "\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "573r1gs9skOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model_lstm, to_file='model.png', show_shapes=True, show_dtype=False, # visualise\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=True, dpi=100,\n",
        ")"
      ],
      "metadata": {
        "id": "7gPgzZRAskRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.compile(optimizer=\"adam\", \n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "iz0henlUtm98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the code below, please, note that it requires quite a lot of time\n"
      ],
      "metadata": {
        "id": "xRoPCf7KhBP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(    \n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    validation_data=(x_test,y_test),            # train model\n",
        "    batch_size=32,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "qbSqSOe2MYut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perfrom ABSA on new data\n"
      ],
      "metadata": {
        "id": "pd-Bo5ri3U-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv(\"path_to_new_data\", usecols=[\"id\", \"text\"]) # load new data\n",
        "data_df.dropna(inplace=True) \n",
        "\n",
        "data_dict = data_df.set_index('id')['text'].to_dict()  # and save as dictionary\n",
        "\n",
        "# preprocess data \n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "split_dict = {} # create dictionary for preprocessed data\n",
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\") \n",
        "\n",
        "for idx, r in data_dict.items():\n",
        "    snts = sent_tokenize(r) # split into sentences\n",
        "    \n",
        "    tknzd_snts = [tokenizer.tokenize(s) for s in snts] # split into words\n",
        "    \n",
        "    snts_wo_sw = [] # create list for words (without stop_words)\n",
        "    \n",
        "    for s in tknzd_snts:\n",
        "        new_s = [w for w in s if not w in stop_words] # save words that are not in stop_words list\n",
        "        snts_wo_sw.append(new_s) # add these words to the list\n",
        "    \n",
        "    max_s_len = 60   # set the maximum length\n",
        "    \n",
        "    final_snts = [] # create list where sentences are tokenized, pads are added and stop-words are removed\n",
        "\n",
        "    for s in snts_wo_sw:   # add pads to sentences that have less than 60 words\n",
        "        new_s = []\n",
        "        for i in range(max_s_len):\n",
        "            try:\n",
        "                new_s.append(s[i])\n",
        "            except:\n",
        "                new_s.append(\"PADword\")\n",
        "        final_snts.append(new_s)\n",
        "    split_dict[idx] = final_snts "
      ],
      "metadata": {
        "id": "Eb0BcvME3TQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the code below, please, note that it requires quite a lot of time"
      ],
      "metadata": {
        "id": "t9bz0rqMc1uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_dict = {}\n",
        "\n",
        "for idx in split_dict:  # get embeddings for each review\n",
        "    print(idx)\n",
        "    vector_dict[idx] = model.get_elmo_vectors(split_dict[idx], layers = 'average')\n",
        "    \n",
        "print(\"done!\")"
      ],
      "metadata": {
        "id": "5OKsPVS03TTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this chunk if you want to use reloaded model (if don't, just skip it)\n"
      ],
      "metadata": {
        "id": "XeVCxjO_dS3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "export_path_keras = \"path_to_model\"\n",
        " \n",
        "reloaded = tf.keras.models.load_model(\n",
        "  export_path_keras,\n",
        "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
        "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
        " \n",
        "reloaded.summary()"
      ],
      "metadata": {
        "id": "ZZqiX50M3TXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = \"s_positive\" # creating sets with tags as they will be needed later\n",
        "neg = \"s_negative\"\n",
        "s_tags = {pos, neg}\n",
        "a_tags = {\"a_exercise\", \"a_material\", \"a_presentation\", \"a_course_arrangement\", \"a_general\"}\n",
        "\n",
        "tags = list(s_tags) + list(a_tags) + [\"O\"]\n",
        "tags.sort()\n",
        "tags"
      ],
      "metadata": {
        "id": "LZ_m4gUp3Tau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = {} \n",
        "\n",
        "for id_r in vector_dict: # predicting tags for embeddings\n",
        "\n",
        "    sent2tags = []\n",
        "    p = model_lstm.predict(vector_dict[id_r])[0] # if you use reloaded model then change 'model_lstm' into 'reloaded'\n",
        "    p = np.argmax(p, axis=-1)\n",
        "    p = p.reshape(1, 60, 1)\n",
        "\n",
        "    for i, sent in enumerate(vector_dict[id_r]):\n",
        "        curr_sent_labels = [] \n",
        "        for n, w, pred in zip(range(len(vector_dict[id_r][i])), vector_dict[id_r][i], p[0]):\n",
        "            curr_sent_labels.append(tags[pred[0]])\n",
        "        sent2tags.append(curr_sent_labels)\n",
        "    all_data[id_r] = sent2tags"
      ],
      "metadata": {
        "id": "fn2ZFK723Tea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "# this function gets sentiment of aspect in each sentence, k - is the sliding window's size (can be changed)\n",
        "# the output is not normalized\n",
        "\n",
        "def count_sentence_sentiment(snt, k=3):\n",
        "    asp2sent = {}\n",
        "    \n",
        "    for a in a_tags:\n",
        "        asp2sent[a] = 0\n",
        "        \n",
        "    i = k  \n",
        "    curr_win = deque(snt[0:k], maxlen=k)\n",
        "    while i <= len(snt):\n",
        "        if (a_tags & set(curr_win)) and (s_tags & set(curr_win)):\n",
        "            curr_s = 0\n",
        "            \n",
        "            for t in curr_win:\n",
        "                if t == pos:\n",
        "                    curr_s += 1\n",
        "                if t == neg:\n",
        "                    curr_s -= 1\n",
        "\n",
        "            for t in curr_win:\n",
        "                if t in a_tags:\n",
        "                    asp2sent[t] += curr_s\n",
        "                    \n",
        "        if i != len(snt):\n",
        "            curr_win.append(snt[i])\n",
        "        i += 1\n",
        "        \n",
        "    return asp2sent\n",
        "\n",
        "# normalization function\n",
        "\n",
        "def normalize_sent(a_sent_dict):\n",
        "    norm_a_sent = {}\n",
        "    for a in a_sent_dict:\n",
        "        if a_sent_dict[a] >= 1:\n",
        "            norm_a_sent[a] = 1\n",
        "        elif a_sent_dict[a] <= -1:\n",
        "            norm_a_sent[a] = -1\n",
        "        else:\n",
        "            norm_a_sent[a] = 0\n",
        "    return norm_a_sent\n"
      ],
      "metadata": {
        "id": "11Z9_D0X3Thl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# this function gets sentiment for aspects in the whole review \n",
        "# the output is not normalized \n",
        "\n",
        "def count_review_sent(review):\n",
        "    counter = Counter()\n",
        "    for s in review:\n",
        "        s_sent = count_sentence_sentiment(s) # get aspects' sentiments for each sentence\n",
        "        s_sent_n = normalize_sent(s_sent) # normalize them\n",
        "        counter.update(s_sent_n) # sum normalized values (but the summary itself is not normalized)\n",
        "    return dict(counter)"
      ],
      "metadata": {
        "id": "9HSVov753Tks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_ids = []\n",
        "res_sent = []\n",
        "texts = []\n",
        "\n",
        "for i, r in all_data.items():\n",
        "#     here values are normalized for the review\n",
        "    norm_sent = normalize_sent(count_review_sent(r))\n",
        "    final_ids.append(i)\n",
        "    texts.append(data_dict[i])\n",
        "    res_sent.append(norm_sent)"
      ],
      "metadata": {
        "id": "X2GXfEoo3TnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a table\n",
        "df_texts = pd.DataFrame({\"id\": final_ids, \"text\": texts})\n",
        "df_sent = pd.DataFrame(res_sent)\n",
        "df_res = pd.concat([df_texts, df_sent], axis=1)\n",
        "\n",
        "df_res"
      ],
      "metadata": {
        "id": "MZSUygGL3Tpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the results"
      ],
      "metadata": {
        "id": "8uQvxFT8gnfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_res.to_excel(\"absa_results.xlsx\", index=False) "
      ],
      "metadata": {
        "id": "0jBzvrFC3TsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
